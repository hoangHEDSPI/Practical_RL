{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/tree/master/sp17_hw/hw2) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "<img src='https://s17.postimg.org/mawroys8f/750px-_Markov_Decision_Process_example.png' width=300px>\n",
    "_img by MistWiz (Own work) [Public domain], via Wikimedia Commons_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:32.642838Z",
     "start_time": "2018-04-02T13:44:32.545142Z"
    }
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.203384Z",
     "start_time": "2018-04-02T13:44:34.199297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('initial state =', 's0')\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.956122Z",
     "start_time": "2018-04-02T13:44:34.949856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mdp.get_all_states =', ('s2', 's1', 's0'))\n",
      "(\"mdp.get_possible_actions('s1') = \", ('a1', 'a0'))\n",
      "(\"mdp.get_next_states('s1', 'a0') = \", {'s2': 0.2, 's1': 0.1, 's0': 0.7})\n",
      "(\"mdp.get_reward('s1', 'a0', 's0') = \", 5)\n",
      "(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", 0.7)\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. For ubuntu just run:\n",
    "\n",
    "1. `sudo apt-get install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:37.797182Z",
     "start_time": "2018-04-02T13:44:37.794073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Graphviz available:', True)\n"
     ]
    }
   ],
   "source": [
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:38.715883Z",
     "start_time": "2018-04-02T13:44:38.648684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: MDP Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"242pt\"\n",
       " viewBox=\"0.00 0.00 720.00 242.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.711137 0.711137) rotate(0) translate(4 337)\">\n",
       "<title>MDP</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-337 1008.46,-337 1008.46,4 -4,4\"/>\n",
       "<!-- s2 -->\n",
       "<g id=\"node1\" class=\"node\"><title>s2</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"40\" cy=\"-137\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"40\" cy=\"-137\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-130.8\" font-family=\"Arial\" font-size=\"24.00\">s2</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>s2&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"192.577\" cy=\"-104\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"192.577\" y=\"-99\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M80.2694,-134.372C100.47,-132.38 125.336,-128.928 147,-123 150.621,-122.009 154.334,-120.786 157.997,-119.443\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"159.476,-122.622 167.479,-115.679 156.894,-116.115 159.476,-122.622\"/>\n",
       "</g>\n",
       "<!-- s2&#45;a0 -->\n",
       "<g id=\"node5\" class=\"node\"><title>s2&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"192.577\" cy=\"-177\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"192.577\" y=\"-172\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a0 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M78.997,-147.094C102.524,-153.343 132.55,-161.32 155.676,-167.463\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"154.882,-170.873 165.445,-170.058 156.679,-164.108 154.882,-170.873\"/>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M165.22,-98.8712C146.167,-96.1868 120.016,-94.6298 98,-101 92.3099,-102.646 86.642,-105.049 81.2097,-107.844\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"79.3177,-104.893 72.3279,-112.855 82.7572,-110.989 79.3177,-104.893\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.5\" y=\"-106.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n",
       "</g>\n",
       "<!-- s1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>s1</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"354.154\" cy=\"-161\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"354.154\" cy=\"-161\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.154\" y=\"-154.8\" font-family=\"Arial\" font-size=\"24.00\">s1</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M218.863,-113.032C242.392,-121.436 278.035,-134.168 306.688,-144.403\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"305.515,-147.7 316.11,-147.768 307.87,-141.108 305.515,-147.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.154\" y=\"-145.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3</text>\n",
       "</g>\n",
       "<!-- s0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>s0</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"824.309\" cy=\"-121\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"824.309\" cy=\"-121\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"824.309\" y=\"-114.8\" font-family=\"Arial\" font-size=\"24.00\">s0</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M220.06,-99.6608C261.077,-93.1825 342.506,-81.2854 412.154,-77 471.597,-73.3425 486.667,-74.1603 546.154,-77 578.779,-78.5573 586.955,-79.5261 619.309,-84 672.722,-91.386 733.479,-102.798 774.694,-110.994\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"774.243,-114.473 784.735,-113.005 775.617,-107.61 774.243,-114.473\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-82.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3 &#160;reward =&#45;1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1 -->\n",
       "<g id=\"node6\" class=\"node\"><title>s1&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"591.731\" cy=\"-194\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"591.731\" y=\"-189\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a1 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M386.482,-185.145C394.444,-190.018 403.272,-194.43 412.154,-197 459.731,-210.766 517.46,-206.186 554.15,-200.794\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"555.033,-204.198 564.37,-199.19 553.948,-197.283 555.033,-204.198\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a0 -->\n",
       "<g id=\"node7\" class=\"node\"><title>s1&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"591.731\" cy=\"-121\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"591.731\" y=\"-116\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a0 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M394.058,-155.644C432.744,-150.141 493.727,-140.987 546.154,-131 548.837,-130.489 551.605,-129.934 554.387,-129.357\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"555.35,-132.73 564.39,-127.206 553.878,-125.887 555.35,-132.73\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a1 -->\n",
       "<g id=\"node8\" class=\"node\"><title>s0&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"976.886\" cy=\"-41\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"976.886\" y=\"-36\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a1 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M859.854,-102.646C884.953,-89.3115 918.697,-71.3835 943.324,-58.2996\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"945.09,-61.3246 952.279,-53.5419 941.806,-55.1429 945.09,-61.3246\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a0 -->\n",
       "<g id=\"node9\" class=\"node\"><title>s0&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"976.886\" cy=\"-154\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"976.886\" y=\"-149\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a0 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M862.139,-134.309C868.795,-136.418 875.715,-138.42 882.309,-140 900.829,-144.437 921.768,-147.726 939.135,-149.998\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"939.077,-153.518 949.434,-151.282 939.943,-146.571 939.077,-153.518\"/>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M220.287,-174.322C243.024,-172.042 276.285,-168.707 303.84,-165.944\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"304.487,-169.397 314.088,-164.917 303.789,-162.432 304.487,-169.397\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.154\" y=\"-178.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.6</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M218.273,-187.937C224.664,-190.499 231.595,-193.047 238.154,-195 271.153,-204.823 280.059,-205.218 314.154,-210 448.782,-228.88 487.498,-264.273 619.309,-231 680.787,-215.481 743.876,-177.388 783.298,-150.392\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"785.607,-153.05 791.826,-144.472 781.615,-147.3 785.607,-153.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-250.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M581.444,-219.85C574.365,-235.385 562.938,-253.861 546.154,-263 493.85,-291.479 471.425,-268.819 412.154,-263 300.706,-252.059 269.307,-254.749 165,-214 134.834,-202.215 103.882,-183.006 80.4481,-166.722\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"82.3571,-163.786 72.17,-160.875 78.3184,-169.503 82.3571,-163.786\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.154\" y=\"-257.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.05</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M566.703,-182.051C560.182,-179.285 553.022,-176.666 546.154,-175 498.994,-163.557 443.523,-160.611 404.547,-160.193\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"404.531,-156.693 394.512,-160.138 404.492,-163.693 404.531,-156.693\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-180.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.95</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M581.343,-95.3326C574.224,-79.8701 562.79,-61.4059 546.154,-52 465.24,-6.25029 324.625,-30.7269 238.154,-44 172.93,-54.0118 155.468,-60.5687 98,-93 91.6378,-96.5904 85.21,-100.834 79.0782,-105.246\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"76.8569,-102.537 70.934,-111.322 81.0423,-108.148 76.8569,-102.537\"/>\n",
       "<text text-anchor=\"middle\" x=\"267.154\" y=\"-49.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.2</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M565.162,-112.684C529.544,-102.544 463.581,-89.0667 412.154,-109 403.441,-112.377 395.243,-117.776 387.928,-123.843\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"385.468,-121.347 380.353,-130.625 390.138,-126.562 385.468,-121.347\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-114.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M619.633,-121C657.484,-121 727.102,-121 774.29,-121\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"774.294,-124.5 784.294,-121 774.294,-117.5 774.294,-124.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"701.809\" y=\"-126.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.7 &#160;reward =5</text>\n",
       "</g>\n",
       "<!-- s0&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>s0&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M951.399,-30.0128C922.053,-17.8681 871.097,-0 825.309,-0 191.577,-0 191.577,-0 191.577,-0 135.108,-0 90.4716,-53.3661 64.8945,-93.484\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"61.8639,-91.73 59.575,-102.075 67.8153,-95.4152 61.8639,-91.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-5.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M965.984,-179.511C946.499,-224.909 898.598,-315 825.309,-315 191.577,-315 191.577,-315 191.577,-315 122.988,-315 79.2258,-237.01 57.4756,-184.277\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"60.6294,-182.737 53.6695,-174.749 54.1289,-185.334 60.6294,-182.737\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.154\" y=\"-320.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M957.286,-134.261C949.896,-127.782 940.876,-121.381 931.309,-118 913.148,-111.582 892.126,-110.912 873.616,-112.362\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"873.226,-108.883 863.624,-113.363 873.923,-115.849 873.226,-108.883\"/>\n",
       "<text text-anchor=\"middle\" x=\"906.809\" y=\"-123.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f114cefb150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, \\\n",
    "                    plot_graph_optimal_strategy_and_state_values\n",
    "        \n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.101416Z",
     "start_time": "2018-04-02T13:43:17.095468Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    Q_value = 0\n",
    "    for s_comma in list(state_values.keys()):\n",
    "        Q_value += mdp.get_transition_prob(state, action, s_comma)*(mdp.get_reward(state, action, s_comma)\n",
    "                                                                   +gamma*state_values[s_comma])\n",
    "    ###YOUR CODE HERE\n",
    "    \n",
    "    return Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.102247Z",
     "start_time": "2018-04-02T13:43:05.502Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69\n"
     ]
    }
   ],
   "source": [
    "print get_action_value(mdp, test_Vs, 's2', 'a1', 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.103358Z",
     "start_time": "2018-04-02T13:43:05.506Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    \n",
    "    ### <YOUR CODE>\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.104340Z",
     "start_time": "2018-04-02T13:43:05.510Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:09.793405Z",
     "start_time": "2018-04-02T13:44:09.770623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "min_difference = 0.001 # stop VI if new values are this close to old values (or closer)\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = <YOUR CODE HERE>\n",
    "    \n",
    "    assert isinstance(new_state_values, dict)\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \"%(i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "    state_values = new_state_values\n",
    "    \n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\"); break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.106395Z",
     "start_time": "2018-04-02T13:43:05.522Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01\n",
    "assert abs(state_values['s1'] - 11.169) < 0.01\n",
    "assert abs(state_values['s2'] - 8.921)  < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.107338Z",
     "start_time": "2018-04-02T13:43:05.525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    \n",
    "    ### <YOUR CODE HERE>\n",
    "    \n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.108149Z",
     "start_time": "2018-04-02T13:43:05.530Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:05.017823Z",
     "start_time": "2018-04-02T13:44:04.962755Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_optimal_strategy_and_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110002Z",
     "start_time": "2018-04-02T13:43:05.538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "    \n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.85 < np.mean(rewards) < 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110991Z",
     "start_time": "2018-04-02T13:43:05.541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.111919Z",
     "start_time": "2018-04-02T13:43:05.545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = <YOUR CODE HERE>\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \"%(i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.112871Z",
     "start_time": "2018-04-02T13:43:05.548Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.114062Z",
     "start_time": "2018-04-02T13:43:05.552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.115092Z",
     "start_time": "2018-04-02T13:43:05.556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    h,w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w,h), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down':(0, -1), 'right':(1,0), 'up':(-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y,x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.116164Z",
     "start_time": "2018-04-02T13:43:05.560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.117143Z",
     "start_time": "2018-04-02T13:43:05.563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.118218Z",
     "start_time": "2018-04-02T13:43:05.568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.119075Z",
     "start_time": "2018-04-02T13:43:05.571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.120316Z",
     "start_time": "2018-04-02T13:43:05.574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.121544Z",
     "start_time": "2018-04-02T13:43:05.578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 - find an MDP for which value iteration takes long to converge  (2+ pts)\n",
    "\n",
    "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
    "\n",
    "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
    "\n",
    "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.122424Z",
     "start_time": "2018-04-02T13:43:05.582Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    " < YOUR CODE >\n",
    "}\n",
    "rewards = {\n",
    " < YOUR CODE >\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.123825Z",
     "start_time": "2018-04-02T13:43:05.586Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "policy = [get_optimal_action(mdp, state_values, state, gamma) \n",
    "          for state in sorted(mdp.get_all_states())]\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    \n",
    "    new_policy = [get_optimal_action(mdp, state_values, state, gamma) \n",
    "                  for state in sorted(mdp.get_all_states())]\n",
    "    \n",
    "    n_changes = np.not_equal(policy, new_policy).sum()\n",
    "    print(\"N actions changed = %i \\n\" % n_changes)\n",
    "    policy = new_policy\n",
    "    \n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 - Policy Iteration (3+ points)\n",
    "\n",
    "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
    "\n",
    "\n",
    "Below are a few helpers that you may or may not use in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.125320Z",
     "start_time": "2018-04-02T13:43:05.590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
    "\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.126518Z",
     "start_time": "2018-04-02T13:43:05.593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.127349Z",
     "start_time": "2018-04-02T13:43:05.597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_policy = {s: np.random.choice(mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "\n",
    "assert type(new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got new state values, it's time to update our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.128415Z",
     "start_time": "2018-04-02T13:43:05.601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.129416Z",
     "start_time": "2018-04-02T13:43:05.604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \"compute_new_policy must return a dict {state : optimal action for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130183Z",
     "start_time": "2018-04-02T13:43:05.608Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" \n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "    < A WHOLE LOT OF YOUR CODE >\n",
    "    \n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your PI Results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130926Z",
     "start_time": "2018-04-02T13:43:05.612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< Compare PI and VI on the MDP from bonus 1, then on small & large FrozenLake >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
